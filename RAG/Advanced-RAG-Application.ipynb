{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"A100","machine_shape":"hm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# Amazon Shoe Department Chat-Bot\n","In this notebook, we will apply a RAG system to the same problem mentioned in the first session. A RAG system is designed to answer questions about shoes that are available in store. The data used here is downloaded from [here](https://data.world/crawlfeeds/amazon-uk-shoes-dataset).\n","\n","This notebook was prepared using some of the material from these two sources:\n","* [Simple RAG for GitHub issues using Hugging Face Zephyr and LangChain](https://colab.research.google.com/github/huggingface/cookbook/blob/main/notebooks/en/rag_zephyr_langchain.ipynb#scrollTo=Kih21u1tyr-I)\n","* [Advanced RAG on Hugging Face documentation using LangChain](https://colab.research.google.com/github/huggingface/cookbook/blob/main/notebooks/en/advanced_rag.ipynb#scrollTo=VjVqmDGh9-9N)"],"metadata":{"id":"L4Y6VlTyR_pA"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"kvq4fPSkrggZ"},"outputs":[],"source":["!pip install -q torch transformers transformers accelerate bitsandbytes langchain langchain-core sentence-transformers openpyxl pacmap datasets langchain-community ragatouille"]},{"cell_type":"code","source":["import os\n","import json\n","import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt"],"metadata":{"id":"tLcKkxaK-tI9"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Downloading Data\n","All the data required for this session are downloaded here."],"metadata":{"id":"DJ7mFiUI15SP"}},{"cell_type":"code","source":["!gdown https://drive.google.com/uc?id=1ZK73jV_3pGbs42gNIo0BYdtpfTcBrTfM"],"metadata":{"id":"G6zVWuZOzalD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!unzip -q /content/data.zip\n","!unzip -q /content/data/images.zip"],"metadata":{"id":"J82e9HeSzjmK"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Loading Product Data\n","We parsed the product data into textual data that can be passed to LLMs."],"metadata":{"id":"7Hri6puC196L"}},{"cell_type":"code","source":["# Loading product data (Show dataset and data preparation)\n","path = \"/content/data/amazon_shoe_database.jsonl\"\n","\n","# Load documents\n","data = []\n"],"metadata":{"id":"nusXY7qZtQMB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["data[0]"],"metadata":{"id":"dWF7uV2bwf3T"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(data[0][\"text\"])"],"metadata":{"id":"yamkNZDR2Hak"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# If you plan to use the LangChain utilities, you can convert the JSON data to\n","# LangChain Documents to integrate with the LangChain library\n","from langchain_core.documents import Document\n","\n","docs = []\n","for item in data:\n","  text = item[\"text\"]\n","  metadata = {key: item[key] for key in [\"url\", \"closure\", \"price\", \"brand\", \"department\"]}\n","  doc = Document.construct(page_content=text, id=item[\"id\"], metadata=metadata)\n","  docs.append(doc)"],"metadata":{"id":"TaoXQQUkxuvD"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Embedding model\n","Here we load the embedding model that we will use to create embeddings for our documents. We will be using [Jina AI's](https://jina.ai/) CLIP V1 model, available on [HuggingFace](https://huggingface.co/jinaai/jina-clip-v1).\n","\n","The embeddings have already been computed earlier and stored in `/content/data/jinaai_doc_embeddings.jsonl`."],"metadata":{"id":"T2A9tjcwENds"}},{"cell_type":"code","source":["!pip install -q einops timm pillow"],"metadata":{"id":"uR0x7l_YHBg-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from transformers import AutoModel\n","\n","# Initialize the model\n","embedding_model_id = 'jinaai/jina-clip-v1'\n","embedding_model = AutoModel.from_pretrained(embedding_model_id, trust_remote_code=True)\n","embedding_model = embedding_model.to('cuda') # Moving to GPU"],"metadata":{"id":"QuMijTRBG1VO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Creating embeddings for our documents\n","# from tqdm import tqdm\n","# save_path = \"doc_embeddings_save_path.jsonl\"\n","\n","# # Read existing doc embeddings\n","# if os.path.exists(save_path):\n","#   with open(save_path, 'r') as f:\n","#     existing_docs = [json.loads(line) for line in f]\n","#   existing_product_ids = set([doc[\"id\"] for doc in existing_docs])\n","# else:\n","#   existing_product_ids = set()\n","\n","# doc_embeddings = []\n","# for doc in tqdm(data):\n","\n","#   product_id = doc[\"id\"]\n","\n","#   if product_id in existing_product_ids:\n","#     continue\n","\n","#   embedding = model.encode_text(doc[\"text\"])\n","\n","#   result = {\n","#       \"product_id\": product_id,\n","#       \"embedding\": embedding.tolist()\n","#   }\n","#   doc_embeddings.append(result)\n","\n","#   with open(save_path, 'a') as f:\n","#     f.write(json.dumps(result) + '\\n')"],"metadata":{"id":"eaiFS5SKNmeP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Loading document embeddings\n","doc_embeddings = {}\n","path = \"/content/data/jinaai_doc_embeddings.jsonl\"\n","with open(path, \"r\") as f:\n","    for line in f:\n","        embedding_info = json.loads(line)\n","        item_id = embedding_info[\"product_id\"]\n","        embedding = embedding_info[\"embedding\"]\n","        doc_embeddings[item_id] = embedding"],"metadata":{"id":"jX6G69tY39Xf"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Vector Database\n","We will directly use the Chroma DB without relying on the LangChain wrapper."],"metadata":{"id":"o980mbeD_yKd"}},{"cell_type":"code","source":["# Installing ChromaDB library\n","!pip install -q chromadb"],"metadata":{"id":"zszsPw_ExyJ-"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["We will create a database using our precomputed embeddings."],"metadata":{"id":"wLmW288sVI8k"}},{"cell_type":"code","source":["import chromadb\n","\n","# Initializing client\n","chroma_client = chromadb.Client()\n","\n","# Creating collection\n","doc_collection = chroma_client.create_collection(\n","    name=\"doc_database\",\n","    metadata={\"hnsw:space\": \"cosine\"} # The similarity measure used to get query neighbours\n","    ) # You can define an embedding function here"],"metadata":{"id":"_ha6OuGZ1CLk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["document_ids = [doc[\"id\"] for doc in data]\n","documents = [doc[\"text\"] for doc in data]\n","metadatas = []\n","\n","for doc in data:\n","  metadata = {key: doc[key] for key in [\"url\", \"closure\", \"price\", \"brand\", \"department\"]}\n","  metadatas.append(metadata)\n","\n","embeddings = [doc_embeddings[doc_id] for doc_id in document_ids]\n","\n","doc_collection.add(\n","    embeddings=embeddings,\n","    documents=documents,\n","    metadatas=metadatas,\n","    ids=document_ids\n",")"],"metadata":{"id":"6n3tS_5XTZqU"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["When retrieving a document from the database, the matching will be made based on the query embedding and the embeddings in the database. The top K documents will be returned with their metadata. Using the metadata, we can provide additional information to the user, such as URLs to the products that the model mentioned."],"metadata":{"id":"A0QYQVqVTwEH"}},{"cell_type":"code","source":["query = \"Men sneakers from Reebok\"\n","query_embedding = embedding_model.encode_text(query)\n","relevant_docs = doc_collection.query(\n","    query_embeddings=[query_embedding.tolist()],\n","    n_results=2 # Number of documents to return\n",")\n","relevant_docs[\"metadatas\"]"],"metadata":{"collapsed":true,"id":"Euj9YrUNTv7a"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Generative Model\n","We will use the same Llama 3 - 8B - Instruct that you saw earlier. However, we will now use it without finetuning and rely on the external database to provide the LLM with the required information about the products."],"metadata":{"id":"xc-sm6WM1tuC"}},{"cell_type":"code","source":["from transformers import pipeline\n","import torch\n","from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n","\n","model_id = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n","\n","bnb_config = BitsAndBytesConfig(\n","    load_in_4bit=True,\n","    bnb_4bit_use_double_quant=True,\n","    bnb_4bit_quant_type=\"nf4\",\n","    bnb_4bit_compute_dtype=torch.bfloat16,\n",")\n","\n","model = AutoModelForCausalLM.from_pretrained(\n","    model_id, quantization_config=bnb_config\n",")\n","tokenizer = AutoTokenizer.from_pretrained(model_id)\n","\n","llm = pipeline(\n","    model=model,\n","    tokenizer=tokenizer,\n","    task=\"text-generation\",\n","    do_sample=True,\n","    temperature=0.2,\n","    repetition_penalty=1.1,\n","    return_full_text=False,\n","    max_new_tokens=500,\n",")"],"metadata":{"id":"JKB2KJ7HKFD7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Testing the model on our domain\n","query = \"What are the best Geox shoes for men?\"\n","out = llm(query)\n","print(out[0][\"generated_text\"])"],"metadata":{"id":"3zCvsJOSM2-E"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The best practice is to use the prompt format specific to the model in use. Llama 3 prompt format can be found [here](https://llama.meta.com/docs/model-cards-and-prompt-formats/meta-llama-3/)."],"metadata":{"id":"JDmbEFps6H18"}},{"cell_type":"code","source":["# Create prompt template based on meta template\n","\n","def get_llama_prompt(query, context):\n","  pass"],"metadata":{"id":"7ftEH8EW6YRz"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Now we will create our own pipeline. LangChain allows for creating pipelines using few code lines. However, if you want to add specific features to your system, it may be more flexible to create your own pipeline."],"metadata":{"id":"IScbbRWMXWkI"}},{"cell_type":"code","source":["# Create RAG pipeline\n","class RAG:\n","    pass"],"metadata":{"id":"XNii7WhcM0J3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["rag_pipeline = RAG(\n","    database=doc_collection,\n","    embedding_model=embedding_model,\n","    model_pipeline=llm,\n","    prompt_template_func=get_llama_prompt,\n",")"],"metadata":{"id":"K4BRmqXGO-dZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Testing the pipeline\n","query = \"Suggest me some Geox shoes for men and give me their prices\"\n","result = rag_pipeline.invoke(query)\n","\n","print(f\"Answer: {result['answer']}\")"],"metadata":{"id":"gM9HkSKUPE63"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Did the model use our data or its internal data?\n","print(\"\\n\\n\".join(result[\"docs\"]))"],"metadata":{"id":"x__HSHEAP23G"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Now we will add an extra feature to the chat-bot. We will load the images of the products that are relevant to the user query and show them to the user."],"metadata":{"id":"G_CQxnNmX5EK"}},{"cell_type":"code","source":["import os\n","from PIL import Image\n","\n","def image_grid(imgs, rows, cols):\n","    \"\"\" Function to show images\"\"\"\n","    assert len(imgs) == rows*cols\n","\n","    w, h = imgs[0].size\n","    grid = Image.new('RGB', size=(cols*w, rows*h))\n","    grid_w, grid_h = grid.size\n","\n","    for i, img in enumerate(imgs):\n","        grid.paste(img, box=(i%cols*w, i//cols*h))\n","    return grid"],"metadata":{"id":"5Y3ncuDoQXTE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Add load images capability to the pipeline\n","class RAG:\n","    def __init__(\n","          self,\n","          database,\n","          embedding_model,\n","          model_pipeline,\n","          prompt_template_func,\n","          n_docs=3\n","          ):\n","        self.database = database\n","        self.embedding_model = embedding_model\n","        self.model_pipeline = model_pipeline\n","        self.get_prompt = prompt_template_func\n","        self.n_docs = n_docs\n","        images_dir = \"/content/images/images\"\n","        self.id2image = {\n","            file_name.split(\".\")[0]: os.path.join(images_dir, file_name) for file_name in os.listdir(images_dir)\n","        }\n","\n","    def get_context(self, docs):\n","        return \"\\n\\n\".join([doc for doc in docs])\n","\n","    def load_images(self, id_list):\n","        image_paths = [self.id2image[product_id] for product_id in id_list if product_id in self.id2image]\n","        images = [Image.open(image_path) for image_path in image_paths]\n","        return images\n","\n","    def invoke(self, query):\n","        query_embedding = self.embedding_model.encode_text(query)\n","        relevant_data = self.database.query(\n","            query_embeddings=[query_embedding.tolist()],\n","            n_results=self.n_docs\n","        )\n","        docs = relevant_data[\"documents\"][0]\n","        ids = relevant_data[\"ids\"][0]\n","        context = self.get_context(docs)\n","        prompt = self.get_prompt(query=query, context=context)\n","        answer = self.model_pipeline(prompt)[0]['generated_text']\n","        images = self.load_images(ids)\n","\n","        # Keep one image for each product\n","        return {'docs': docs, 'prompt': prompt, 'answer': answer, \"images\": images}"],"metadata":{"id":"lPZMc4B4QotJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["rag_pipeline = RAG(\n","    database=doc_collection,\n","    embedding_model=embedding_model,\n","    model_pipeline=llm,\n","    prompt_template_func=get_llama_prompt,\n",")"],"metadata":{"id":"0LGjn2i8RkNt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["query = \"Suggest me some Reebok shoes for men and give me their prices\"\n","result = rag_pipeline.invoke(query)\n","\n","print(f\"Answer: {result['answer']}\")\n","if result[\"images\"]:\n","  display(image_grid(result[\"images\"], 1, len(result[\"images\"])))"],"metadata":{"id":"j7Ek3FKRQQIQ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Using image embeddings\n","The reason that we used the [Jina AI](https://jina.ai/) CLIP model is that it is a multimodal (text + image) embedding model. A multimodal embedding model can produce embeddings for images and text that are semantically comparable, i.e., an embedding of an image of a cat will be similar to the embedding of the word \"cat\"."],"metadata":{"id":"4R9T6DLNWQSw"}},{"cell_type":"code","source":["from PIL import Image\n","\n","# Loading example images\n","dir_path = \"/content/data/example_images/animal_images/\"\n","image_files = os.listdir(dir_path)\n","images = [Image.open(os.path.join(dir_path, file)) for file in image_files]\n","\n","# Displaying example images\n","fig, axs = plt.subplots(1, len(images), figsize=(15, 5))\n","for i, image in enumerate(images):\n","    axs[i].imshow(image)\n","    axs[i].axis('off')\n","plt.show()"],"metadata":{"id":"O7PJsRczYfZK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.metrics.pairwise import cosine_similarity\n","\n","query = \"An image of a cat in a grass field\"\n","\n","query_embedding = embedding_model.encode_text(query)\n","image_embeddings = []\n","similarities = []\n","for image in images:\n","    image_embedding = embedding_model.encode_image(image)\n","    image_embeddings.append(image_embedding)\n","    similarity = cosine_similarity([query_embedding], [image_embedding])[0][0]\n","    similarities.append(similarity)\n","\n","similarities = np.array(similarities)\n","order_by_similarity = similarities.argsort()[::-1]\n","ordered_images = [images[i] for i in order_by_similarity][:3]\n","\n","fig, axs = plt.subplots(1, len(ordered_images), figsize=(15, 5))\n","for i, image in enumerate(ordered_images):\n","    axs[i].imshow(image)\n","    axs[i].axis('off')\n","\n","plt.show()"],"metadata":{"id":"jwKp_g56YfZL"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["We already computed the embeddings for the images earlier. We load them here:"],"metadata":{"id":"FVsqDUTUCEpv"}},{"cell_type":"code","source":["img_embeddings = {}\n","path = \"/content/data/jinaai_image_embeddings.jsonl\"\n","with open(path, \"r\") as f:\n","    for line in f:\n","        embedding_info = json.loads(line)\n","        item_id = embedding_info[\"product_id\"]\n","        embedding = embedding_info[\"embedding\"]\n","        img_embeddings[item_id] = embedding"],"metadata":{"id":"kXadgYJGWQOc"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["We will now create a database using the embeddings of the images:"],"metadata":{"id":"Oed3UOYVjBWf"}},{"cell_type":"code","source":["import chromadb\n","\n","# Initializing client\n","chroma_client = chromadb.Client()\n","\n","# If a collection already exists you must delete it\n","# chroma_client.delete_collection(\"img_database\")\n","\n","# Creating collection\n","img_collection = chroma_client.create_collection(\n","    name=\"img_database\",\n","    metadata={\"hnsw:space\": \"cosine\"} # The similarity measure used to get query neighbours\n","    ) # You can define an embedding function here"],"metadata":{"id":"kFAgBQNrZEs0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["document_ids = [product_id for product_id in img_embeddings.keys()]\n","docs_by_id = {doc[\"id\"]: doc for doc in data}\n","documents = [docs_by_id[doc_id][\"text\"] for doc_id in document_ids]\n","metadatas = []\n","\n","for doc_id in document_ids:\n","  doc = docs_by_id[doc_id]\n","  metadata = {key: doc[key] for key in [\"url\", \"closure\", \"price\", \"brand\", \"department\"]}\n","  metadatas.append(metadata)\n","\n","embeddings = [img_embeddings[doc_id] for doc_id in document_ids]\n","\n","img_collection.add(\n","    embeddings=embeddings,\n","    documents=documents,\n","    metadatas=metadatas,\n","    ids=document_ids\n",")"],"metadata":{"id":"DNNXLyyYZEs1"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Now we can query this database by embedding the user query and searching for similar image embeddings in the database. This only works with multimodal models that can project images and text to the same latent space."],"metadata":{"id":"pHshQkGcZvKh"}},{"cell_type":"code","source":["# A dictionary that maps each product ID to its image path\n","images_dir = \"/content/images/images/\"\n","id2image = {\n","    file_name.split(\".\")[0]: os.path.join(images_dir, file_name) for file_name in os.listdir(images_dir)\n","}"],"metadata":{"id":"AnRp9_uujryI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["query = \"I want a yellow and blue sneaker\"\n","\n","query_embedding = embedding_model.encode_text(query)\n","relevant_docs = img_collection.query(\n","    query_embeddings=[query_embedding.tolist()],\n","    n_results=2 # Number of documents to return\n",")\n","\n","product_ids = relevant_docs[\"ids\"][0]\n","image_paths = [id2image[product_id] for product_id in product_ids]\n","images = [Image.open(product_image) for product_image in image_paths]\n","\n","image_grid(images, 1, len(images))"],"metadata":{"id":"aC9sD0lLaeUM"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["We can also search for products that look like a certain image:"],"metadata":{"id":"n8ziSzVJbqNe"}},{"cell_type":"code","source":["# Searching by image\n","search_image_path = \"/content/data/example_images/shoe_images/banana_shoes.jpeg\"\n","# search_image_path = \"/content/data/example_images/shoe_images/clown_shoes.jpg\"\n","search_image = Image.open(search_image_path)\n","search_image\n"],"metadata":{"id":"MgnBjTWmbvnO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["search_image_embedding = embedding_model.encode_image(search_image)\n","relevant_docs = img_collection.query(\n","    query_embeddings=[search_image_embedding.tolist()],\n","    n_results=2 # Number of documents to return\n",")\n","\n","product_ids = relevant_docs[\"ids\"][0]\n","\n","image_paths = [id2image[product_id] for product_id in product_ids]\n","images = [Image.open(product_image) for product_image in image_paths]\n","\n","image_grid(images, 1, len(images))"],"metadata":{"id":"Og_Ya5UCcuyO","collapsed":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Improving the RAG system"],"metadata":{"id":"hsEE3urtkpIZ"}},{"cell_type":"markdown","source":["### Evaluation standard\n","In order to be able to improve the RAG system, we first need to know how to evaluate its performance. Evaluation of LLMs on downstream tasks is an extremely hard task. The reason is that unlike classification or regrassion, the output is not structured (text) and there is no single true \"label\". I strongly suggest listening to [this talk](https://www.youtube.com/watch?v=2CIIQ5KZWUM) to learn about challenges in evaluating LLMs on downstream tasks."],"metadata":{"id":"C2eCDbbp9v0n"}},{"cell_type":"markdown","source":["### Filtering metadata\n","We can narrow down our search by filtering based some fields in the metadata, such as search products in a certain price range or from a certain brand. This is a trivial feature in all databases; however, integrating it with a conversational bot is not trivial. One way would be to have an agent, i.e., an LLM, to extract information from the user query and use it to filter the search."],"metadata":{"id":"CjbZg6S4yvU2"}},{"cell_type":"code","source":["query = \"Men sneakers from Reebok\"\n","query_embedding = embedding_model.encode_text(query)\n","relevant_docs = doc_collection.query(\n","    query_embeddings=[query_embedding.tolist()],\n","    n_results=2, # Number of documents to return\n","    where={\"department\": \"men\"}\n",")\n","relevant_docs[\"metadatas\"]"],"metadata":{"id":"cyHzo-ylkveX"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Reranking documents\n","The vector databases use approximate approaches to ensure fast retrieval. A common approach is to retrieve more documents than required and then rerank them using a better retrieval approach. [Colbertv2](https://arxiv.org/abs/2112.01488) is a good choice for this purpose. It can be easily used through [the RAGatouille library](https://github.com/bclavie/RAGatouille)."],"metadata":{"id":"N_RT2mqr1HGN"}},{"cell_type":"code","source":[],"metadata":{"id":"kYz4yHvL3lre"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Finetuning\n","A recent paper [RAFT: Adapting Language Model to Domain Specific RAG](https://arxiv.org/abs/2403.10131) showed improved performance by finetuning an LLM on answering questions given documents. Specifically, they show noticable improvement in performance when using chain-of-thought reasoning when finetuning. Read more about chain-of-thought reasoning [here](https://www.promptingguide.ai/techniques/cot)."],"metadata":{"id":"jKU5971o2E9c"}},{"cell_type":"code","source":[],"metadata":{"id":"bgExVtqP3c-X"},"execution_count":null,"outputs":[]}]}