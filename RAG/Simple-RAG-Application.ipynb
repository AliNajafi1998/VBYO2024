{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# Implementing a simple RAG pipeline\n","In this notebook, we will be implementing a simple RAG pipeline using LangChain library. Our RAG system's purpose is to answer student's questions about course offerings in Sabanci University.\n","\n","This notebook was prepared using some of the material from these two sources:\n","* [Simple RAG for GitHub issues using Hugging Face Zephyr and LangChain](https://colab.research.google.com/github/huggingface/cookbook/blob/main/notebooks/en/rag_zephyr_langchain.ipynb#scrollTo=Kih21u1tyr-I)\n","* [Advanced RAG on Hugging Face documentation using LangChain](https://colab.research.google.com/github/huggingface/cookbook/blob/main/notebooks/en/advanced_rag.ipynb#scrollTo=VjVqmDGh9-9N)"],"metadata":{"id":"m5KbOzf8CapM"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"bYrVyzmyDc6V"},"outputs":[],"source":["# Downloading relevant libraries\n","!pip install -q torch transformers accelerate bitsandbytes transformers sentence-transformers langchain-chroma langchain langchain-community langchain-huggingface"]},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n"],"metadata":{"id":"yS5GQ4HIsbK7"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Loading Data\n","The data we will be using was scraped from the [Sabanci Student Information system](https://suis.sabanciuniv.edu/prod/bwckschd.p_disp_dyn_sched)."],"metadata":{"id":"geXxDf4y2RZO"}},{"cell_type":"code","source":["!gdown https://drive.google.com/uc?id=1oNPMb2rzlPd3HOi64k1Nz4uOhPnYshAs\n","!unzip -q /content/sabanci_course_docs.zip"],"metadata":{"id":"G2xin2zDFBvu"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["LangChain provides document loaders that allow loading different types of data such as:\n","* Text\n","* PDF\n","* HTML\n","* CSV\n","* Markdown\n","* File Directory\n","\n","You can check all these loaders [here](https://python.langchain.com/v0.1/docs/modules/data_connection/document_loaders/). Since our data is text files, we will use the `TextLoader`."],"metadata":{"id":"OgQ_3oBmLJdu"}},{"cell_type":"code","source":["# Loading data\n","from langchain_community.document_loaders import TextLoader\n","import glob\n","\n","docs = []\n","for file in glob.glob(\"/content/sabanci_course_docs/*.txt\"):\n","  # Initialize loader\n","\n","\n","  # Load documents\n","\n","\n","print(f\"No. Documents: {len(docs)}\")"],"metadata":{"id":"6_-DoYX9rTse"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Checking document lengths\n","print([len(doc.page_content) for doc in docs])"],"metadata":{"id":"f-7SLYhFsYrl"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Processing documents\n","\n","\n","  <div>\n","  <img src=\"https://drive.google.com/uc?export=view&id=1y_9TBlqTeVDdLUanNmVulQWrzs0A1OgO\" width=\"500\"/>\n","  </div>\n","\n","[Image source](https://www.linkedin.com/pulse/ragparadigm-mohamed-azharudeen/)\n","\n","Now that we have the documents loaded as text, we need to split them to small chunks (**Why?** *Because we want to create a database of text-chunk-embeddings that will be used to find the parts of the document that are relevant to the user query*). When splitting documents into chunks, we need to take into consideration several factors:\n","1. **Maximum chunk length**: This depends on:\n","  * **Embedding model**: The length of each chunk should not exceed the maximum sequence length of the model that is used to create the embeddings of the document chunks.\n","  * **Generative model**: Since we will be passing $k$ of the document chunks as context to the generative LLM, we need to make sure that the maximum sequence length of the generative LLM allows for fitting the prompt template, the context ($k$ document chunks), the query, and the model response.\n","\n","2. **Semantic content in chunks**: Ideally, we want each chunk to have a specific information content so that we can easily match it with the user query. However, it is hard to control the semantic content of the chunk since we are automatically splitting the documents. Having very large chunks will result in several ideas being covered in the same chunk, which will force the embedding to represent all these ideas, making it harder to match it with the user query. The choice on how to split the document into chunks highly depends on the type of documents.\n","\n","The LangChain `text_splitter` module provides several utilities for splitting the text into smaller chunks ([link](https://python.langchain.com/docs/modules/data_connection/document_transformers))."],"metadata":{"id":"MKFdjgkz3rxh"}},{"cell_type":"markdown","source":["1. **Split by character using `CharacterTextSplitter`:**\n","This is the simplest method. It splits using a specific character defined by the user, such as `\\n`. The user also defines a `chunk_size` parameter as an upper bound to the chunk size. `CharacterTextSplitter` splits the text using the defined character and keeps merging until it gets the longest text that does not excced the defined `chunk_size`. However, if a part of the splitted text exceeds the `chunk_size` it does not split it any further, resulting in a chunk that is longer than the maximum `chunk_size`.\n","\n","The `chunk_overlap` argument allows for keeping an overlap between chunks. Since we are not sure that each chunk captures a specific meaning in the document, having an overlap between chunks increases the chances of capturing the semantic meanings in one chunk."],"metadata":{"id":"RSGXT2XvNHm5"}},{"cell_type":"markdown","source":["2. **Recursively split by character `RecursiveCharacterTextSplitter`**: This solves the problem of the first splitter by allowing splits using the more than one character. Ex: we can first split paragraphs, then if a paragraph exceeds the maximum `chunk_size` we split by sentences, and finally we split by words if necessary. This way we can ensure to meet the `chunk_size` limit."],"metadata":{"id":"w4tB4gp_OMgq"}},{"cell_type":"markdown","source":["Check [this site](https://chunkviz.up.railway.app/) to see how the different parameters of a `text_splitter` work."],"metadata":{"id":"WKKyoWbbOPpX"}},{"cell_type":"code","source":["from langchain.text_splitter import RecursiveCharacterTextSplitter\n","\n","# Initialize splitter with chunk_size, chunk_overlap, and separators\n","splitter =\n","\n","# Split documents\n","chunked_docs =\n","\n","print(f\"No. Chunks: {len(chunked_docs)}\")"],"metadata":{"id":"x1BK8mgu1daP","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1720877819087,"user_tz":-180,"elapsed":774,"user":{"displayName":"yasser zouzou","userId":"12641901960277579713"}},"outputId":"0d94e1bc-4887-4856-b91f-c7b99e01e671"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["No. Chunks: 371\n"]}]},{"cell_type":"code","source":["# Show sample chunk"],"metadata":{"id":"NeqUOagm6DFw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Let us visualize the length of the chunks distribution\n","\n","# Compute lengths of chunks\n","len_arr =\n","\n","# Visualize chunk size histogram\n","plt.hist(len_arr, color=\"gray\", alpha=0.7, bins=20)\n","plt.title(\"Chunk Length Distribution\")\n","plt.xlabel(\"Chunk Length (No. Characters)\")\n","plt.ylabel(\"Frequency\")\n","plt.show()"],"metadata":{"id":"M6YhLrcB4Lgs"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["How can we make sure that the chunk lengths are suitable for the mbedding model? **We need to measure their length by token length**. The embedding model that we will be using accepts a maximum of 512 tokens."],"metadata":{"id":"B6Lx6sf_O3xd"}},{"cell_type":"code","source":["from transformers import AutoTokenizer\n","\n","embedding_model_id = 'BAAI/bge-base-en-v1.5'\n","\n","# Load model tokenizer from pretrained tokenizer\n","tokenizer ="],"metadata":{"collapsed":true,"id":"O5pys_s6PLUM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Tokenizer reminder\n","text = \"Hello this is a textual message\"\n","token_ids =\n"],"metadata":{"id":"h1fWgG7xPWH4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Create a function to compute chunk size by token length\n","def get_num_tokens(text):\n","  pass"],"metadata":{"id":"dRR2GCJYP3YX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","# Compute lengths of chunks\n","len_arr =\n","\n","plt.hist(len_arr, color=\"gray\", alpha=0.7, bins=20)\n","plt.axvline(512, color=\"red\", linestyle=\"--\", lw=2)\n","plt.title(\"Chunk Length Distribution\")\n","plt.xlabel(\"Chunk Length (No. Characters)\")\n","plt.ylabel(\"Frequency\")\n","plt.show()"],"metadata":{"id":"V804pnZ5QCbE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Initialize splitter with chunk_size, chunk_overlap, separators, and length_function\n","splitter =\n","\n","# Split documents\n","chunked_docs =\n","\n","print(f\"No. Chunks: {len(chunked_docs)}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aNBmw--LQS-w","executionInfo":{"status":"ok","timestamp":1720877825030,"user_tz":-180,"elapsed":720,"user":{"displayName":"yasser zouzou","userId":"12641901960277579713"}},"outputId":"aec9c0ee-8ab9-499f-c857-988ecb0c5f96"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Token indices sequence length is longer than the specified maximum sequence length for this model (535 > 512). Running this sequence through the model will result in indexing errors\n"]},{"output_type":"stream","name":"stdout","text":["No. Chunks: 393\n"]}]},{"cell_type":"code","source":["len_arr = [get_num_tokens(chunk.page_content) for chunk in chunked_docs]\n","plt.hist(len_arr, color=\"gray\", alpha=0.7, bins=20)\n","plt.axvline(512, color=\"red\", linestyle=\"--\", lw=2)\n","plt.title(\"Chunk Length Distribution\")\n","plt.xlabel(\"Chunk Length (No. Characters)\")\n","plt.ylabel(\"Frequency\")\n","plt.show()"],"metadata":{"id":"Wj9_BzesQjPN"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Text embedding\n","* Extensive introduction to sentence embeddings\n","* [Text Embedding - LangChain](https://python.langchain.com/docs/modules/data_connection/text_embedding/)\n","* Follow the steps here to get an API key to use Google Gemini: [Google Gemini quick start](https://ai.google.dev/tutorials/python_quickstart)\n","\n","Now that we have divided our text into chunks, we need to pass these chunks into an embedding model to get their respective embeddings. We can use any LLM that provides embeddings to sentences. This includes models on the cloud accessible through API, such as OpenAI ChatGPT and Google Gemini, or local models. You can see the models supported by the LangChain library [here](https://python.langchain.com/docs/integrations/text_embedding). The choice of the embedding model is usually limited by the following factors:\n","* **Privacy concerns:** If we are dealing with private data, regulations may not allow us to send the data to commercial LLM services such as ChatGPT or Gemini.\n","* **Available computational power:** If we opt to use local models, we need to choose a model that can be run on the available hardware.\n","* **Latency:** Which is the time required to process a query and generate embeddings. See this article for more details on the latency of LLMs ([link](https://www.databricks.com/blog/llm-inference-performance-engineering-best-practices)).\n","* **Context length**: Which is the maximum number of tokens that a model can process. This is more of a concern for the generating LLm rather than the embedding LLM since we usually embed the documents at the sentence level."],"metadata":{"id":"qhUL6qKmN6jd"}},{"cell_type":"markdown","source":["1. **Using LLM API**\n","\n","Check [tutorial](https://ai.google.dev/tutorials/python_quickstart) on how to setup and use Gemini API."],"metadata":{"id":"vV8LV6kthBF9"}},{"cell_type":"code","source":["!pip install -q -U langchain-google-genai\n","!pip install -q -U google-generativeai"],"metadata":{"id":"kjnFjg_aMG-x"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import google.generativeai as genai\n","from google.colab import userdata\n","import os\n","\n","GOOGLE_API_KEY = userdata.get('GOOGLE_API_KEY')\n","os.environ['GOOGLE_API_KEY'] = GOOGLE_API_KEY\n","genai.configure(api_key=GOOGLE_API_KEY)"],"metadata":{"id":"QHomhJs5O7aI"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["LangChain provides wrappers for several embedding models. Each of these classes have the methods `embed_query` and `embed_document` to compute embeddings for queries and documents, respectively."],"metadata":{"id":"e9O_LtkxVVLF"}},{"cell_type":"code","source":["from langchain_google_genai import GoogleGenerativeAIEmbeddings\n","\n","embedding_model_id = \"models/embedding-001\"\n","\n","# Initialize embedding model\n","embedding_model =\n","\n","# Computing query embeddings using embed_query method\n","embedding_vector =\n","\n","print(len(embedding_vector))\n","embedding_vector[:5]"],"metadata":{"id":"mbLTUMwoQn2W"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Example\n","query = \"What is the capital of America?\"\n","chunks = [\n","    \"Washington D.C. is the capital of the USA\",\n","    \"Rates of return on invested capital were high\",\n","    \"American football is popular in India\",\n","    \"The White House is in Washington, D.C.\",\n","    \"Cats are better pets than dogs\"\n","]\n","\n","# Computing query embeddings using embed_query method\n","query_embedding =\n","\n","# Computing documents embeddings using embed_documents method\n","chunk_embeddings ="],"metadata":{"id":"eAJ-DbQBUN-i"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Computing similarities\n","from sklearn.metrics.pairwise import cosine_similarity\n","\n","for chunk, chunk_embedding in zip(chunks, chunk_embeddings):\n","  similarity = cosine_similarity([query_embedding], [chunk_embedding])[0][0]\n","  print(f\"{chunk}: {similarity:.3f}\")"],"metadata":{"id":"y-l0J0t2QrZY"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["2. **Using a local model:**\n","\n","For the local model, we will use Sentence-BERT ([link to original paper](https://arxiv.org/abs/1908.10084)) to generate embeddings. Several pretrained models of this structure are provided through the `sentence_transformers` library ([link to documentation](https://www.sbert.net/))."],"metadata":{"id":"Ewn5vAgliRxJ"}},{"cell_type":"code","source":["from langchain_huggingface import HuggingFaceEmbeddings\n","embedding_model_id = 'BAAI/bge-base-en-v1.5'\n","\n","# Initialize embedding model with model_kwargs={'device': 'cuda'} to load model on GPU\n","embedding_model ="],"metadata":{"id":"xJbDsxDt7hJl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Example\n","query = \"What is the capital of America?\"\n","chunks = [\n","    \"Washington D.C. is the capital of the USA\",\n","    \"Rates of return on invested capital were high\",\n","    \"American football is popular in India\",\n","    \"The White House is in Washington, D.C.\",\n","    \"Cats are better pets than dogs\"\n","]\n","\n","# Computing query embeddings using embed_query method\n","query_embedding =\n","\n","# Computing chunks embeddings using embed_documents method\n","chunk_embeddings ="],"metadata":{"id":"8x0EJPBo7lR1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.metrics.pairwise import cosine_similarity\n","\n","for chunk, chunk_embedding in zip(chunks, chunk_embeddings):\n","  similarity = cosine_similarity([query_embedding], [chunk_embedding])[0][0]\n","  print(f\"{chunk}: {similarity:.3f}\")"],"metadata":{"id":"u0cZlj9J8US6"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Vector Stores\n","Now that we have created embeddings for the document chunks, we need to store them in a database that allows us to quickly retrieve the most similar text chunk embeddings to a given query embedding. A vector store is a database type that is used to save high dimensional numerical vectors, i.e., embeddings. These databases allow for very fast retrieval of the most similar $k$ vectors to a query vector. The speed of retrieval is attributed to the usage of approximate nearest neighbour (ANN) alogorithms rather than the expensive K-nearest neighbour algorithm.\n","\n","LangChain integrates several open-source vector stores ([link](https://python.langchain.com/docs/modules/data_connection/vectorstores/))."],"metadata":{"id":"LJCufSJjXBKm"}},{"cell_type":"code","source":["from langchain_chroma import Chroma\n","\n","# Initialize database instance using from_documents method and chunked documents and embedding model\n","db ="],"metadata":{"id":"b2KuxlSd5sU8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Creating retriever from the database instance\n","retriever = db.as_retriever(\n","    search_type=\"similarity\",\n","    search_kwargs={'k': 6}\n",")"],"metadata":{"id":"dg0WRUas7BPI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Test the retriever using the invoke method\n","query = \"Who is the instructor of the Intro to Data Science course in Fall 2023?\"\n","relevant_docs =\n","\n","# Show relevant documents"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Eo8GSW_i9n0S","executionInfo":{"status":"ok","timestamp":1720878609390,"user_tz":-180,"elapsed":655,"user":{"displayName":"yasser zouzou","userId":"12641901960277579713"}},"outputId":"cbd1feb9-5e98-4c7e-84b5-dcac19fc1331"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Computer Science courses\n","Introduction to Data Science Recitation - 10093 - CS 210R - D\n","Associated Term: Fall 2023-2024 \n","Registration Dates:  No dates available  \n","Levels: Doctorate, Undeclared, Scientific Preparatory, Undergraduate, Masters, Special Student, Exchange - Socrates Erasmus UG, Exchange - Socrates Erasmus MA, Exchange - Socrates Erasmus DR, Exchange - Erasmus Mundus MA, Exchange - Erasmus Mundus DR, Exchange - Erasmus Mundus UG \n","Faculty: \n","Course Offered by FENS\n","Attributes: Lang. of Instruction: English, Course Offered by FENS \n","Sabancı University Campus Campus\n","Recitation Schedule Type\n","       0.000 Credits\n","|    | Type   | Time              | Days   | Where                             | Date Range                  | Schedule Type   | Instructors                                                                             |\n","|---:|:-------|:------------------|:-------|:----------------------------------|:----------------------------|:----------------|:----------------------------------------------------------------------------------------|\n","|  0 | Class  | 4:40 pm - 6:30 pm | T      | Fac. of Engin. and Nat. Sci. G035 | Oct 02, 2023 - Jan 05, 2024 | 1st del         | Onur   Varol (P), Kerem   Aydın , Damla   Erden , Halil İbrahim  Ergül , Mansur   Kiraz |\n"]}]},{"cell_type":"markdown","source":["## Generative model\n","The final step is to have a generative LLM that we can chat with. In this tutorial we will use Google's Gemini model. You can check the API rate limits and pricing [here](https://ai.google.dev/pricing)."],"metadata":{"id":"nMaiLO12_NBG"}},{"cell_type":"code","source":["import google.generativeai as genai\n","\n","for model in genai.list_models():\n","  print(model.name)"],"metadata":{"id":"7Fszd2NwA_pG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from langchain_google_genai import ChatGoogleGenerativeAI\n","\n","model_id = \"models/gemini-1.5-flash\"\n","\n","# Initialize generative model\n","llm =\n","\n","# Test generative model using the invoke method\n","query = \"What is RAG?\"\n"],"metadata":{"id":"APCcvMQxAmwp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Asking about courses at Sabanci\n","query = \"Who teaches the Deep Learning graduate course at sabanci university?\"\n","result ="],"metadata":{"id":"65Gh8NpfA7Sy"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Final RAG pipeline\n"],"metadata":{"id":"rp989JlZBgEi"}},{"cell_type":"code","source":["from langchain.prompts import PromptTemplate\n","from transformers import pipeline\n","from langchain_core.output_parsers import StrOutputParser\n","from langchain_core.runnables import RunnablePassthrough\n","\n","# Create prompt template\n","prompt_template =\n","\n","prompt_generator = PromptTemplate(\n","    input_variables=[\"context\", \"question\"],\n","    template=prompt_template,\n",")"],"metadata":{"id":"Q4Q_EJSNBUKn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Test prompt template using the invoke method\n","input_data = {\"context\": \"This is a relevant document\", \"question\": \"This is the user question?\"}\n","prompt =\n","\n","print(prompt.text)"],"metadata":{"id":"TkCgu5NbCOLw"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### RAG Chain\n","LangChain allows chaining several components into one pipeline using a simple syntax. The input to the chain is processed by each part of the chain and then passed to the next part. In our case, the processing order is:\n","\n","1. The user query is sent to the `retriever` to get relevant documents.\n","2. The documents are passed through the `format_docs` to convert them into one string.\n","3. The question is passed through `RunnablePassthrough()`, which just takes the input and passes it to the next part of the chain. We use it here to pass the user query to the prompt generator. At this point, we have the input to the prompt generator ready (context + question).\n","4. The context and question are sent to the prompt generator, which place them into the prompt template that we defined.\n","5. The prompt is sent to the LLM\n","6. The output of the LLM is parsed using `StrOutputParser()`.\n"],"metadata":{"id":"WT1P18eraufF"}},{"cell_type":"code","source":["def format_docs(docs):\n","    return \"\\n\\n\".join(doc.page_content for doc in docs)\n","\n","\n","rag_chain = (\n","    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()} # Steps 1, 2, 3\n","    | prompt_generator # Step 4\n","    | llm # Step 5\n","    | StrOutputParser() # Step 6\n",")"],"metadata":{"id":"mu7yM_nzZvJi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import textwrap\n","\n","from IPython.display import display\n","from IPython.display import Markdown\n","\n","\n","def to_markdown(text):\n","  text = text.replace('•', '  *')\n","  return Markdown(textwrap.indent(text, '> ', predicate=lambda _: True))"],"metadata":{"id":"tS8Txk_LE3TW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["question = \"When are the CS 514 course classes? Who teaches the course?\"\n","# question = \"Suggest me some economics courses in Fall 2023 on Mondays\"\n","# question = \"Are there any machine learning courses offered in Fall 2023?\"\n","# question = \"What is the best lab in Sabanci University?\"\n","\n","output = rag_chain.invoke(question)\n","to_markdown(output)"],"metadata":{"id":"cnD5JhVdDB_P"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"fI74U7SIxfAd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"d3OR8PqzF1tl"},"execution_count":null,"outputs":[]}]}